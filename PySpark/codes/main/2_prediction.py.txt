
'''
###########################################################
# Workflow Klassifikation in Emere
###########################################################

INPUT: 
  - Array mit Aufbau:
      DF = ['gtin',
                'artikel_id', 
                'artikelname',  
                'variable_zur_naeheren_beschreibung_der_artikel', 
                'unternehmensinterne_klassifizierung', # Warengruppen
                'inhaltsmenge', 
                'masseinheit', 
                'verpackungstyp', 
                'mehrwertsteuersatz',
                'handelsketten_id']
  - Transformer/Objekte zum Klassifizieren:
    - BestModel (trainiertes Modell mit getuned'en Parametern)
    - Pipeline: alle Prozessee die zur Erstellung des finalen feature strings notwendig sind,
      konkret:
      STAGES in pipeline:
        - RegexTokenizer
        - NGram (trigram)
        - CountVectorizer
        - IDF
        - StringIndexer
        - VectorAssembler
    - Warengruppen Look-Up Table
OUTPUT: 
  - DF mit Spalten ["p","coicop"]
  
  
TODO: ReFactoring: Nutze Konventionen aus Emere 

'''

# Lade Pakete
from pyspark.sql import SparkSession
from pyspark.sql.functions import countDistinct
from pyspark.sql.functions import concat, concat_ws,col
from pyspark.sql.functions import lower, col, lit, when
from pyspark.sql.functions import regexp_replace
from pyspark.sql.functions import SparkContext
from pyspark.sql.functions import udf

from pyspark.sql.functions import udf, array_max
from pyspark.sql.types import (StructType, 
                               StructField, 
                               StringType, 
                               FloatType, 
                               IntegerType, 
                               ArrayType, 
                               DoubleType)
#import pyspark.sql.types as T

## ML
#from pyspark.ml.functions import vector_to_array
from pyspark.ml.linalg import Vector, Vectors
from pyspark.ml.feature import ChiSqSelector
from pyspark.ml.feature import HashingTF, IDF

# from pyspark.mllib.feature import HashingTF, IDF
from pyspark.ml.feature import (Tokenizer, RegexTokenizer,
                                VectorAssembler,
                                CountVectorizer,
                                IDF,
                                StringIndexer,
                                IndexToString,
                                NGram)
from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml import Pipeline, PipelineModel
from pyspark import SparkContext
import numpy as np

### NEU ###
from klassifikation_eigeneModule import CreateSelectionCol,RegexDf 
from emere_config import *

# Lade Session
# Einrichtung der Spark-Session
spark = SparkSession \
    .builder \
    .appName("Emere_Datenaufbereitung") \
    .enableHiveSupport() \
    .config("spark.sql.crossJoin.enabled", "true") \
    .getOrCreate()

sc=spark.sparkContext

# Konfiguration und Parameter Settings
# -----------------------------------------------------
# ID = 999 bezieht sich auf die polnischen Scannerdaten
# Wenn das Modell mit default-Parametern trainiert werden soll, dann einfach tuning
# auf "False" setzen

# Pfade und Konfiguraionsdateien

df_tmp = spark.read.text(f"{HDFS_ROOT}{EMERE_KONFIGURATION}{KONFIGURATION_KLASSIFIKATOR}")
t = df_tmp.collect() # DF zu Liste

# Erstelle Dictionary mit allen Konfigurations-Parametern
confList = {}
for d in t: confList[d.value.split(':')[0]] = d.value.split(':')[1].replace(' ','')
  
df_tmp.unpersist() # Speicher freigeben

# Extrahiere Parameter damit diese separat abgerufen werden können
delim = confList['trennzeichen']
seeed = int(confList['random_seed'])
p_list = [float(x) for x in confList['akzeptanzschwellwerte'].split(',')]
shop_list = [str(x) for x in confList['handelsketten_id'].split(',')]
keine_wg_klassifikation = [str(x) for x in confList['keine_warengruppen_klassifikation'].split(',')]
keine_ml_klassifikation = [str(x) for x in confList['keine_ml_klassifikation'].split(',')]
Xcol = confList['bezeichner_feature_daten']

# Pfade (AUCH AUS CONFIG DATEI ZIEHEN)
ssp = '118'
data_test = f'{KLASSIFIKATION}{TRAININGSDATEN}{ssp}_TDS.csv'


## Lade die Pre-Processing Pipeline
# ---------------------------------------------------------------
# Erstelle Dictionary, damit jeder Handelskette ein Schwellenwert zugewiesen werden kann. 
# Achtung: Die Länge von "shop_list" und "p_list" müssen identisch sein, anders funktionieren 
# die multiplen Iterationen nicht
p_min = {}
for shp, pshp in zip(shop_list, p_list):
  p_min[str(shp)] = pshp

# Initialisiere ein dictionary, damit jeder Handelskette Module zugewiesen werden kann
modules = {}

for shp in shop_list:
  if shp in confList: 
    # Für Testzwecke
    datPipe = confList[str(shp)].split(',')[0]
    dataMod = confList[str(shp)].split(',')[1]
    dataWg = confList[str(shp)].split(',')[2] 
  else:
    datPipe = confList['default_klassifikator'].split(',')[0]
    dataMod = confList['default_klassifikator'].split(',')[1]
    dataWg = confList['default_klassifikator'].split(',')[2]
  
  
# Manche Handelsketten haben keine brauchbaren Warengruppen, die müssen abgefangen/ausgeleitet werden
  if str(shp) in keine_wg_klassifikation:
    wgs = None
  else:
    wgs = spark.read.csv(f'{KLASSIFIKATION}{WARENGRUPPEN}{dataWg}', sep = delim, header=True)
    
# Für die Shops wo wir alles manuell klassifizieren
  if str(shp) in keine_ml_klassifikation:
    piLi = None
    bestModel = None
  else:
    piLi =  PipelineModel.load(f'{KLASSIFIKATION}{TRANSFORMER}{datPipe}')
    bestModel = RandomForestClassificationModel.load(f'{KLASSIFIKATION}{TRANSFORMER}{dataMod}')
  
## Modul Index:         0       1        2    
  modules[str(shp)] = [piLi, bestModel, wgs]           


## Lade Testdaten (INPUT_DATA)
# ---------------------------------------------------------------
df_klassifikation = spark.read.csv(data_test, sep = delim, header=True) 
df_klassifikation = df_klassifikation.withColumnRenamed('artikel_name', 'artikelname')
df_klassifikation = df_klassifikation.withColumnRenamed('haendlerkategorie', 'unternehmensinterne_klassifizierung')
df_klassifikation = df_klassifikation.dropDuplicates(['artikel_id'])

# TMP
df_klassifikation = df_klassifikation.withColumn('handelsketten_id', lit(ssp))

## Handelsketten-IDS isolieren
# ---------------------------------------------------------------


# Initialisiere den finalen DataFrame
cols = ['artikel_id','artikelname','unternehmensinterne_klassifizierung','handelsketten_id', 'p','coicop']
schema = StructType([
  StructField(cols[0], StringType(), True),
  StructField(cols[1], StringType(), True),
  StructField(cols[2], StringType(), True),
  StructField(cols[3], StringType(), True),
  StructField(cols[4], StringType(), True),
  StructField(cols[5], StringType(), True)
  ])
df_ergebnis_klassifiziert = spark.createDataFrame([], schema)
#df_klassifikation = spark.createDataFrame([], schema) # zum testen und bug-fixing

# Filter die "handelsketten_id" und iteriere über diese
shop = df_klassifikation.groupBy(['handelsketten_id']).count()
for shp in shop.select('handelsketten_id').collect():
  shop_id = str(shp[0])

  # Erstelle DF für jeden Dataframe, damit nicht bei jedem Abruf ein Filter gesetzt werden muss  
  df_shopData = df_klassifikation.filter(df_klassifikation['handelsketten_id']== shop_id)

  ## Lade Test-Warengruppen
  # ---------------------------------------------------------------
  if shop_id in keine_wg_klassifikation: # oder modules[shop_id][2] == None
    # Falls keine Warengruppen für die Klassifikation verwertbar sind, erstelle leeren DataFrame 
    warengruppen_zuordnung = spark.createDataFrame([], schema)
  else:  
    warengruppen_zuordnung = modules[shop_id][2]
    # Spaltennamen anpassen
    warengruppen_zuordnung = warengruppen_zuordnung.withColumnRenamed('haendlerkategorie', 'unternehmensinterne_klassifizierung')
    warengruppen_zuordnung = warengruppen_zuordnung.withColumnRenamed('ECOICOP', 'coicop')
    # Reduziere auf die notwendigen Spatlen
    warengruppen_zuordnung = warengruppen_zuordnung.select(['unternehmensinterne_klassifizierung', 'coicop']) 

    # Bevor es zuvor vergessen wurde: Hier werden nochmal alle Duplikate der
    # klassifizierten Händlerkategorien entfernt, damit es beim "Mergen" keine Probleme gibt
    warengruppen_zuordnung = warengruppen_zuordnung.dropDuplicates(['unternehmensinterne_klassifizierung'])

    ## Merge/Join
    # ---------------------------------------------------------------
    warengruppen_zuordnung = df_shopData.join(warengruppen_zuordnung,on='unternehmensinterne_klassifizierung',how='inner')\
    .orderBy('unternehmensinterne_klassifizierung', ascending=True)

    # Spalte mit der Wahrscheinlichkeit 'p' hinzufügen - im "double" Format
    warengruppen_zuordnung = warengruppen_zuordnung.withColumn('p', lit('1').cast('double'))    
  
  # Für die Shops wo wir alles manuell klassifizieren
  if shop_id in keine_ml_klassifikation:
    predictedLabel = df_shopData.join(warengruppen_zuordnung, on = 'artikel_id', how = 'left_anti')
    predictedLabel = predictedLabel.withColumn('coicop',  lit(None))
    predictedLabel = predictedLabel.withColumn('p',  lit(None))
  else:    
    # Bestimme die Artikel, die in den ML-Part geleitet werden
    mlData = df_shopData.join(warengruppen_zuordnung, on = 'artikel_id', how = 'left_anti')
    # Erstelle feature string durch Konkatenation
    mlData = mlData.select(concat_ws(' ',mlData.artikelname,
                                     mlData.unternehmensinterne_klassifizierung).alias(Xcol),'*') 
  
    df_shopData.unpersist()

  
    ## Transformiere die Daten und schätze die Coicops
    # -----------------------------------------------------
    if len(mlData.head(1)) == 0:
      predictedLabel = mlData
      if 'p' not in predictedLabel.columns:
          predictedLabel = predictedLabel.withColumn('p',lit(None))
      if 'coicop' not in predictedLabel.columns:
          predictedLabel = predictedLabel.withColumn('coicop',lit(None))
      
    else:
      predictedLabel = modules[shop_id][0].transform(mlData) # pre-process Pipeline
      predictedLabel = modules[shop_id][1].transform(predictedLabel) # Modell
  
    # Lade Converter aus der Pipeline hinaus
      labels = modules[shop_id][0].stages[5]
      converter = IndexToString(inputCol='prediction', outputCol='coicop',labels=labels.labels)
      predictedLabel = converter.transform(predictedLabel)

      mlData.unpersist() # Speicher freigeben
  

      # Transformiere DenseVector zu Array
      # INFO: ab Spark 3.0.0 gibt es dies als inbuild-Command und kann mit "vector_to_array" ausgeführt werden.
      # ALTERNATIV: UDF erstellen und über die Dörfer gehen
      to_array = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))
      predictedLabel = predictedLabel.withColumn('p', array_max(to_array('probability')))
      predictedLabel = predictedLabel.drop('probability')

  
    ## Ausleitung vorbereiten durch COICOP-Löschung
    # ------------------------------------------------------------------------
    # Identifiziere alle Artikel bei denen die Korrektklassifikations-Wahrscheinlichkeit
    # unter einer gewissen Grenze liegt (p_min) und überschreibe die COICOP mit "NICHTS"
      predictedLabel = predictedLabel.withColumn('coicop', 
                                      when(predictedLabel.p <p_min[shop_id], None)
                                     .otherwise(predictedLabel.coicop))
  # Hier Spaltenbezeichnung anpassen
  predictedLabel = predictedLabel.select(warengruppen_zuordnung.columns).union(warengruppen_zuordnung)
  warengruppen_zuordnung.unpersist()
  df_ergebnis_klassifiziert = df_ergebnis_klassifiziert.union(predictedLabel.select(cols))
  predictedLabel.unpersist() # Speicher freigeben

df_klassifikation.unpersist()
shop.unpersist()
df_ergebnis_klassifiziert.show()
